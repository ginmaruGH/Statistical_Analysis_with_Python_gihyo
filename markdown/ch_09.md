# Chapter09 独立同一分布

- 確率変数が互いに独立であるとは、
  - 確率変数がほかの確率変数に影響を及ぼさないこと。
- 独立同一分布（independently and identically distributed, i.i.d.）
  - お互いに独立で、さらにそれぞれが同じ確率分布に従う多次元確率変数のこと
  - それらが従う確率分を$F$とすると、
    - $X_1, X_2, ..., X_n \sim {i.i.d.} ~ F$
  - と表される。
  - 「同じ条件下で行われる実験や観測を複数回繰り返すことでデータを得る」ということを、数学の言葉で表したもの
  - 統計解析においてもっとも基本的でかつ重要な条件設定。

## 9.1 | 独立性

### 9.1.1 独立性の定義

- 確率変数の独立性（independence）とは
  - 2つ以上の確率変数が互いに影響を及ぼさず無関係であることを表す概念

2次元確率変数$(X, Y)$の場合、次のような関係が成り立つことを、**独立である**という。

$$
f_{X, Y}(x, y) = f_X(x)f_Y(y)
$$

すなわち、確率変数が独立なとき、同時確率は周辺確率の積で書くことができる。

#### 独立性

n個の確率変数 $X_1, X_2, ..., X_n$ が、

$$
f_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n) =
f_{X_1}(x_1)f_{X_2}(x_2)...f_{X_n}(x_n)
$$

を満たすとき、$X_1, X_2, ..., X_n$は互いに独立であるという。

ただし、関数$f$は、

- 離散型であれば、確率関数
- 連続型であれば、密度関数

を表す。

---

### 9.1.2 独立性と無相関性

- 共分散や相関係数が「0」のとき、
  - 2つの確率変数の間に、相関性がないことを表している。
  - 相関性がないとは、直線的な関係がないこと。

- 無相関性より独立性のほうが強い概念
  - 2つの確率変数$XとY$が「独立なとき」、$XとY$は「無相関」。
  - 2つの確率変数$XとY$が「無相関なとき」、$XとY$は「必ずしも独立とはなっていない」。
    - 2つの確率変数の間に直線的な関係はないものの、影響を及ぼし合う場合がある。

---
---

&nbsp;

## 9.2 | 和の分布

- 和の分布
  - 互いに独立に同一の確率分布に従う「確率変数 $X_1, X_2, ..., X_n$」の<br>
  「和 $\sum_{i=1}^{n} X_i = X_1 + X_2 + ... + X_n$」が従う確率分布のこと

### 確率変数の和の期待値

和の分布の期待値は、それぞれの確率変数の期待の和で計算できる。

確率変数 $X_1, X_2, ..., X_n$ について、

$$
E(X_1 + X_2 + ... + X_n) = E(X_1) + E(X_2) + ... + E(X_n)
$$

が成り立つ。

※この公式は確率変数が独立でなくても成り立つ。

### 確率変数の和の分散

和の分布の分散も、それぞれの確率変数の分散の和で計算できる。

確率変数 $X_1, X_2, ..., X_n$ が互いに独立ならば、

$$
V(X_1 + X_2 + ... + X_n) = V(X_1) + V(X_2) + ... + V(X_n)
$$

が成り立つ。

### 9.2.1 正規分布の和の分布

- 正規分布に関しては、正規分布の和もまた正規分布になるという性質をもっている。
- 再生性
  - 同じ確率分布に従う2つの独立な確率変数に対して、その和もまた同じ確率分布になる性質のこと。
  - 再生性はすべての確率分布がもつ性質ではない。

#### 正規分布の和の分布

互いに独立な確率変数
$X_1 \sim N(\mu_1, \sigma_1^2), ~ X_2 \sim N(\mu_2, \sigma_2^2), ..., ~ X_n \sim N(\mu_n, \sigma_n^2),$ について、

$$
\sum_{i=1}^{n} X_i \sim
N \left( \sum_{i=1}^{n}\mu_i, ~ \sum_{i=1}^{n}\sigma_i^2 \right)
$$

---

### 9.2.2 ポアソン分布の和の分布

#### ポアソン分布の和の分布

互いに独立な確率変数
$X_1 \sim Poi(\lambda_1), ~ X_2 \sim Poi(\lambda_2), ..., ~ X_n \sim Poi(\lambda_n),$ について、

$$
\sum_{i=1}^{n} X_i \sim
Poi \left( \sum_{i=1}^{n}\lambda_i \right)
$$

が成り立つ。

---

### 9.2.3 ベルヌーイ分布の和の分布

- ベルヌーイ分布には、再生性はない。
- ベルヌーイ分布の和は二項分布になる。
- 二項分布のパラメータは、「$n, p$」。

#### ベルヌーイ分布の和の分布

$X_1, X_2, ..., X_10 \sim i.i.d ~ Bern(p)$ について、

$$
\sum_{i=1}^{n} X_i \sim Bin(n, p)
$$

が成り立つ。

---
---

&nbsp;

## 9.3 | 標本平均の分布

- 標本平均の分布とは、
  - 互いに独立に同一の確率分布に従う確率変数 $X_1, X_2, ..., X_n$ の標本平均 $\overline{X} = \frac{X_1 + X_2 + ... + X_n}{n}$ が従う分布。
  - 母平均の区間推定や、母平均の検定で使う分布。

標本平均の期待値は、期待値の線形性を使って計算する。

$$
\begin{align*}
E (\overline{X})
&= E \Big( \dfrac{X_1 + X_2 + ... + X_n}{n} \Big) \\
&= \dfrac{E(X_1) + E(X_2) + ... + E(X_n)}{n} \\
&= \dfrac{n \mu}{n} \\
&= \mu
\end{align*}
$$

標本平均の分散は、期待値とは異なり、$V(aX) = a^2V(X)$ となる。

$$
\begin{align*}
V(\overline{X})
&= V \Big(\dfrac{X_1 + X_2 + ... + X_n}{n}\Big) \\
&= \dfrac{V(X_1) + V(X_2) + ... + V(X_n)}{n^2} \\
&= \dfrac{n \sigma^2}{n^2} \\
&= \dfrac{\sigma^2}{n}
\end{align*}
$$

### 標本平均の期待値と分散

確率変数 $X_1, X_2, ..., X_n$ が互いに独立に、期待値が $\mu$ で分散が $\sigma^2$ の確率分布 $F$ にしたがっているとき、

$$
\begin{align*}
E(\overline{X}) &= \mu \\
V(\overline{X}) &= \dfrac{\sigma^2}{n} \\
\end{align*}
$$

が成り立つ。

### 9.3.1 正規分布の標本平均の分布

#### 正規分布の標本平均の分布

$X_1, X_2, ..., X_n \sim i.i.d. ~ N(\mu, \sigma^2)$ としたとき、

$$
\overline{X} \sim N(\mu, \dfrac{\sigma^2}{n})
$$

が成り立つ。

---

### 9.3.2 ポアソン分布の標本平均の分布

---

### 9.3.3 中心極限定理

- 中心極限定理（central limit theorem）
  - 確率変数 $X_1, X_2, ..., X_n$ が互いに独立に、
  - 期待値が $\mu$ で分散が $\sigma^2$ の確率分布 $F$ にしたがっているとき、
  - $n$ が大きくなるにつれ標本平均 $\overline{X}$ の分布は正規分布 $N(\mu, \sigma^2/n)$ に近づく。

---

### 9.3.4 大数の法則

- 大数の法則（law of large numbers）
  - サンプルサイズを大きくすると、標本平均は母平均に収束することを主張している定理。
  - 確率変数 $X_1, X_2, ..., X_n$ が互いに独立に、
  - 平均が $\mu$ で分散が $\sigma^2$ であるような確率分布にしたがっているとき、
  - $n$ が大きくなるにつれ標本平均は $\mu$ に収束していく。

---
---

&nbsp;
