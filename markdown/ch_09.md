# Chapter09 独立同一分布

- 確率変数が互いに独立であるとは、
  - 確率変数がほかの確率変数に影響を及ぼさないこと。
- 独立同一分布（independently and identically distributed, i.i.d.）
  - お互いに独立で、さらにそれぞれが同じ確率分布に従う多次元確率変数のこと
  - それらが従う確率分を$F$とすると、
    - $X_1, X_2, ..., X_n \sim {i.i.d.} ~ F$
  - と表される。
  - 「同じ条件下で行われる実験や観測を複数回繰り返すことでデータを得る」ということを、数学の言葉で表したもの
  - 統計解析においてもっとも基本的でかつ重要な条件設定。

## 9.1 | 独立性

### 9.1.1 独立性の定義

- 確率変数の独立性（independence）とは
  - 2つ以上の確率変数が互いに影響を及ぼさず無関係であることを表す概念

2次元確率変数$(X, Y)$の場合、次のような関係が成り立つことを、**独立である**という。

$$
f_{X, Y}(x, y) = f_X(x)f_Y(y)
$$

すなわち、確率変数が独立なとき、同時確率は周辺確率の積で書くことができる。

#### 独立性

n個の確率変数 $X_1, X_2, ..., X_n$ が、

$$
f_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n) =
f_{X_1}(x_1)f_{X_2}(x_2)...f_{X_n}(x_n)
$$

を満たすとき、$X_1, X_2, ..., X_n$は互いに独立であるという。

ただし、関数$f$は、

- 離散型であれば、確率関数
- 連続型であれば、密度関数

を表す。

---

### 9.1.2 独立性と無相関性

- 共分散や相関係数が「0」のとき、
  - 2つの確率変数の間に、相関性がないことを表している。
  - 相関性がないとは、直線的な関係がないこと。

- 無相関性より独立性のほうが強い概念
  - 2つの確率変数$XとY$が「独立なとき」、$XとY$は「無相関」。
  - 2つの確率変数$XとY$が「無相関なとき」、$XとY$は「必ずしも独立とはなっていない」。
    - 2つの確率変数の間に直線的な関係はないものの、影響を及ぼし合う場合がある。

---
---

&nbsp;

## 9.2 | 和の分布

- 和の分布
  - 互いに独立に同一の確率分布に従う「確率変数 $X_1, X_2, ..., X_n$」の<br>
  「和 $\sum_{i=1}^{n} X_i = X_1 + X_2 + ... + X_n$」が従う確率分布のこと

### 確率変数の和の期待値

和の分布の期待値は、それぞれの確率変数の期待の和で計算できる。

確率変数 $X_1, X_2, ..., X_n$ について、

$$
E(X_1 + X_2 + ... + X_n) = E(X_1) + E(X_2) + ... + E(X_n)
$$

が成り立つ。

※この公式は確率変数が独立でなくても成り立つ。

### 確率変数の和の分散

和の分布の分散も、それぞれの確率変数の分散の和で計算できる。

確率変数 $X_1, X_2, ..., X_n$ が互いに独立ならば、

$$
V(X_1 + X_2 + ... + X_n) = V(X_1) + V(X_2) + ... + V(X_n)
$$

が成り立つ。

### 9.2.1 正規分布の和の分布

---

### 9.2.2 ポアソン分布の和の分布

---

### 9.2.3 ベルヌーイ分布の和の分布

---
---

&nbsp;

## 9.3 | 標本平均の分布

### 9.3.1 正規分布の標本平均の分布

---

### 9.3.2 ポアソン分布の標本平均の分布

---

### 9.3.3 中心極限定理

---

### 9.3.4 大数の法則

---
---

&nbsp;
